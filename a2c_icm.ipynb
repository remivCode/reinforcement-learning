{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variables and testing environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.seed = RANDOM_STATE\n",
    "torch.manual_seed(torch.seed)\n",
    "np.random.seed(torch.seed)\n",
    "random.seed(torch.seed)\n",
    "\n",
    "GAMMA = 0.99\n",
    "BETA = 0.2\n",
    "LAMBDA = 0.4\n",
    "LR = 0.0001\n",
    "\n",
    "NUM_FEAT_SPACE = 256\n",
    "\n",
    "GRADIENT_CLIPPING_ICM = 0.5\n",
    "GRADIENT_CLIPPING_A2C = 0.5\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "MAX_MOVES = 218\n",
    "\n",
    "ENV_NAME = 'ALE/Bowling-ram-v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: \n",
      "[ 71 255   0   0   0   0   0   0   0   0   0   0 255   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   8  15   0   0   0 170   0\n",
      "   1   0   0   0   1  12 184 247   0   0   6  16  19  13  22  16  10  25\n",
      "  19  13   7   7   5   5   3   3   3   1   1   1   1   0   0   0   0   0\n",
      "   0   0   0   0   0 136 216 132  38  88   0   0   1 255   0 255 128 255\n",
      "   0   0   0   0   0   0   0   2   2   0   8   8   0  34  34   0 136 136\n",
      "   0  34  34   0   8   8   0   2   2   0   0   0   0   0   0   0   0   0\n",
      "  66 243]\n",
      "Observation space: \n",
      "Box(0, 255, (128,), uint8)\n",
      "Action space: Discrete(6)\n",
      "Output from applying action 0 on environment:\n",
      "state:[ 75 255   0   0   0   0   4   4   4   4   4   4 255   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   8  15   0   2   0 170   0\n",
      "   1   0   0   0   1  12 184 247   0   0   0  16  19  13  22  16  10  25\n",
      "  19  13   7   7   5   5   3   3   3   1   1   1   1   0   0   0   0   0\n",
      "   0   0   0   0   0 136 216 132  38  88   0   0   1 255   0 255   0 134\n",
      "   0   0   0   0   0   0   0   2   2   0   8   8   0  34  34   0 136 136\n",
      "   0  34  34   0   8   8   0   2   2   0   0   0   0   0   0   0   0   0\n",
      "  52   7]\n",
      "reward: 0.0\n",
      "done: False\n",
      "truncated: False\n",
      "info: {'lives': 0, 'episode_frame_number': 4, 'frame_number': 4}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "state, _ = env.reset()\n",
    "print(f\"Initial state: \\n{state}\")\n",
    "print(f\"Observation space: \\n{env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "a = env.action_space.sample()\n",
    "event = env.step(a)\n",
    "print('Output from applying action {} on environment:\\nstate:'.format(a) \\\n",
    "      + '{}\\nreward: {}\\ndone: {}\\ntruncated: {}\\ninfo: {}'.format(*event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FeatureEncoderNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncoderNet(torch.nn.Module):\n",
    "    \"\"\" Network for feature encoding\n",
    "\n",
    "        In: [s_t]\n",
    "            Current state (i.e. pixels) -> 1 channel image is needed\n",
    "\n",
    "        Out: phi(s_t)\n",
    "            Current state transformed into feature space\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(FeatureEncoderNet, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        # Set the environment seed\n",
    "        self.env.seed = torch.seed\n",
    "\n",
    "        # constants\n",
    "        self.numStateSpace = self.env.observation_space.shape[0]\n",
    "        self.numHidden1 = NUM_FEAT_SPACE\n",
    "\n",
    "        # layers\n",
    "        self.lstm = torch.nn.LSTMCell(input_size=self.numStateSpace, hidden_size=self.numHidden1)\n",
    "\n",
    "          \n",
    "    def reset_lstm(self, x):\n",
    "        self.h_t1 = self.c_t1 = torch.zeros(x, self.numHidden1).cuda() if torch.cuda.is_available() else torch.zeros(x, self.numHidden1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h_t1, self.c_t1 = self.lstm(x, (self.h_t1, self.c_t1)) # h_t1 is the output\n",
    "        return self.h_t1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inverse Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseNet(torch.nn.Module):\n",
    "    \"\"\" Network for the inverse dynamics\n",
    "\n",
    "        In: torch.cat((phi(s_t), phi(s_{t+1}), 1)\n",
    "            Current and next states transformed into the feature space, \n",
    "            denoted by phi().\n",
    "\n",
    "        Out: \\hat{a}_t\n",
    "            Predicted action\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(InverseNet, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        # Set the environment seed\n",
    "        self.env.seed = torch.seed\n",
    "\n",
    "        # constants\n",
    "        self.numHidden1 = 256\n",
    "        self.numFeatSpace = NUM_FEAT_SPACE\n",
    "        self.numActionSpace = self.env.action_space.n\n",
    "\n",
    "        # layers\n",
    "        #self.conv = ConvBlock()\n",
    "        self.layer1 = torch.nn.Linear(self.numFeatSpace * 2, self.numHidden1)\n",
    "        self.layer2 = torch.nn.Linear(self.numHidden1, self.numActionSpace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.layer1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Forward Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardNet(torch.nn.Module):\n",
    "    \"\"\" Network for the forward dynamics\n",
    "\n",
    "    In: torch.cat((phi(s_t), a_t), 1)\n",
    "        Current state transformed into the feature space, \n",
    "        denoted by phi() and current action\n",
    "\n",
    "    Out: \\hat{phi(s_{t+1})}\n",
    "        Predicted next state (in feature space)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ForwardNet, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        # Set the environment seed\n",
    "        self.env.seed = torch.seed\n",
    "\n",
    "        # constants\n",
    "        self.numHidden = 256\n",
    "        self.numFeatSpace = NUM_FEAT_SPACE \n",
    "        self.numActionSpace = self.env.action_space.n\n",
    "\n",
    "        # layers\n",
    "        #self.conv = ConvBlock()\n",
    "        self.layer1 = torch.nn.Linear(self.numFeatSpace + self.numActionSpace, self.numHidden)\n",
    "        self.layer2 = torch.nn.Linear(self.numHidden, self.numFeatSpace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.layer1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ICM Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICMNet(torch.nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ICMNet, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        # Set the environment seed\n",
    "        self.env.seed = torch.seed\n",
    "\n",
    "        # networks\n",
    "        self.featureEncoderNet = FeatureEncoderNet(self.env)\n",
    "        self.forwardNet = ForwardNet(self.env)\n",
    "        self.inverseNet = InverseNet(self.env)\n",
    "\n",
    "    def forward(self, s_t, s_t1, a_t):\n",
    "        \"\"\"\n",
    "            s_t : current state\n",
    "            s_t1: next state\n",
    "\n",
    "            phi_t: current encoded state\n",
    "            phi_t1: next encoded state\n",
    "\n",
    "            a_t: current action\n",
    "        \"\"\"\n",
    "\n",
    "        phi_t = self.featureEncoderNet(s_t)\n",
    "        phi_t1 = self.featureEncoderNet(s_t1)\n",
    "\n",
    "        # forward dynamics\n",
    "        # predict next encoded state\n",
    "        forward = torch.cat((phi_t, a_t), 1) # concatenate next to each other\n",
    "        phi_t1_pred =  self.forwardNet(forward)\n",
    "\n",
    "        # inverse dynamics\n",
    "        # predict the action between s_t and s_t1\n",
    "        inverse = torch.cat((phi_t, phi_t1), 1)\n",
    "        a_t_pred = self.inverseNet(inverse)\n",
    "\n",
    "        return phi_t1, phi_t1_pred, a_t_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A2C Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CNet(torch.nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(A2CNet, self).__init__()\n",
    "        # Setting up the environment\n",
    "        self.env = env\n",
    "        # Set the environment seed\n",
    "        self.env.seed = torch.seed\n",
    "        # Store the size of the action and observation space\n",
    "        self.numFeatSpace = NUM_FEAT_SPACE\n",
    "        self.numActionSpace = self.env.action_space.n\n",
    "\n",
    "        # Model for the feature encoder\n",
    "        self.featureEncoderNet = FeatureEncoderNet(env)\n",
    "\n",
    "        # Create a model for the actor - policy\n",
    "        self.actorNet = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.numFeatSpace, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, self.numActionSpace),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        # Create a model for the critic - value\n",
    "        self.criticNet = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.numFeatSpace, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, s_t):\n",
    "        \"\"\"\n",
    "            s_t : current state\n",
    "           \n",
    "            phi_t: current encoded state\n",
    "        \"\"\"\n",
    "        phi_t = self.featureEncoderNet(s_t)\n",
    "\n",
    "        policy = self.actorNet(phi_t)\n",
    "        value = self.criticNet(phi_t)\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, env, num_epoch, num_steps=MAX_MOVES, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "\n",
    "        # constants\n",
    "        self.is_cuda = torch.cuda.is_available()\n",
    "        self.env = env\n",
    "        self.env.seed = torch.seed\n",
    "        self.numActionSpace = self.env.action_space.n\n",
    "        self.num_epoch = num_epoch\n",
    "        self.maxMoves = num_steps\n",
    "        self.batchSize = batch_size\n",
    "        \n",
    "        self.clear_log_lists()\n",
    "\n",
    "        # networks\n",
    "        self.icmNet = ICMNet(self.env)\n",
    "        self.a2cNet = A2CNet(self.env)\n",
    "\n",
    "        if self.is_cuda:\n",
    "            self.icmNet.cuda()\n",
    "            self.a2cNet.cuda()\n",
    "\n",
    "        # loss\n",
    "        self.criticLoss = torch.nn.MSELoss()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.Adam(list(self.icmNet.parameters()) + list(self.a2cNet.parameters()), lr=LR)\n",
    "        \n",
    "    def clear_log_lists(self):\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.policies = []\n",
    "        self.values = []  \n",
    "            \n",
    "\n",
    "      \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "            s_t : current state\n",
    "            s_t1: next state\n",
    "\n",
    "            phi_t: current encoded state\n",
    "            phi_t1: next encoded state\n",
    "\n",
    "            a_t: current action\n",
    "        \"\"\"\n",
    "                \n",
    "        # reset all logger lists\n",
    "        self.clear_log_lists()\n",
    "        \n",
    "        self.a2cNet.featureEncoderNet.reset_lstm(1)\n",
    "\n",
    "        # play one game\n",
    "        state, _  = self.env.reset()\n",
    "        self.states.append(torch.from_numpy(state))\n",
    "\n",
    "        done = False\n",
    "\n",
    "        self.score = 0\n",
    "\n",
    "        for step in range(self.maxMoves):\n",
    "            policy, value = self.a2cNet(torch.from_numpy(\n",
    "                        state).float().unsqueeze(0)) # select action from the policy\n",
    "            \n",
    "            action = np.random.choice(self.numActionSpace, p=policy[0].detach().numpy())\n",
    "            \n",
    "            # interact with the environment\n",
    "            nextState, reward, done, truncated,info = self.env.step(action)\n",
    "\n",
    "            self.score += reward\n",
    "\n",
    "            _, nextValue = self.a2cNet(torch.from_numpy(nextState).float().unsqueeze(0))\n",
    "\n",
    "            self.actions.append(action)\n",
    "            self.policies.append(policy)\n",
    "            self.values.append(value) # nextValue - value ?\n",
    "            self.states.append(torch.from_numpy(nextState))\n",
    "            self.rewards.append(reward)\n",
    "            self.replay.append((state, reward, nextValue.item()))\n",
    "            \n",
    "            if done:\n",
    "              break\n",
    "\n",
    "            state = nextState\n",
    "\n",
    "\n",
    "\n",
    "    def normalize(self, data):\n",
    "      return (data - data.mean()) / (data.std() + 10e-9)\n",
    "    \n",
    "      \n",
    "      \n",
    "    def a2c_loss(self):      \n",
    "      # Policy Loss\n",
    "      # Calculate the advantage\n",
    "      advantages = torch.Tensor(list(self.rewards)).float() + torch.pow(GAMMA, torch.arange(\n",
    "          len(self.values)).float()) * torch.Tensor(list(self.values)).float()\n",
    "\n",
    "      # Store the state info as a batch of states\n",
    "      stateBatch = torch.stack([s.float() for s in self.states])[0:-1]\n",
    "      \n",
    "      # Store the action info as a batch of actions    \n",
    "      actionBatch = torch.Tensor(list(self.actions))\n",
    "      # Feed the state batch to the actor model to calculate the probs of actions for each state in the batch\n",
    "      self.a2cNet.featureEncoderNet.reset_lstm(stateBatch.shape[0])\n",
    "      policy, _ = self.a2cNet(stateBatch)\n",
    "      # Gets the probs of actions actually performed for each state\n",
    "      probs = policy.gather(\n",
    "          dim=1, index=actionBatch.long().unsqueeze(dim=1)).squeeze()\n",
    "\n",
    "      # Policy Loss\n",
    "      actorLoss = - 1 * torch.log(probs) * advantages\n",
    "\n",
    "      if (len(self.replay) > self.batchSize):\n",
    "        # Select a set of random indices to be chosen from the replay buffer \n",
    "        indices = np.random.choice(len(self.replay), size=self.batchSize)\n",
    "        # Extract the experiences from the replay buffer\n",
    "        replay_ = np.asarray(self.replay, dtype=object)[indices, :]  \n",
    "        # Create a state batch with the excted experiences\n",
    "        stateBatch = torch.stack([torch.from_numpy(s).float()\n",
    "                                  for s in replay_[:, 0]])\n",
    "        # Calculate the value for the extracted states      \n",
    "        self.a2cNet.featureEncoderNet.reset_lstm(stateBatch.shape[0])                                  \n",
    "        _, value = self.a2cNet(stateBatch) \n",
    "        # Critic Loss\n",
    "        criticLoss = torch.nn.MSELoss()(value.squeeze(), torch.Tensor(list(replay_[:, 1] + GAMMA * replay_[:, 2])).float())\n",
    "        \n",
    "      else:\n",
    "        criticLoss = torch.tensor(0)\n",
    "      # return the a2c loss\n",
    "      # which is the sum of the actor (policy) and critic (advantage) losses\n",
    "      loss = actorLoss.sum() + criticLoss.sum()\n",
    "      \n",
    "      return loss\n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            s_t : current state\n",
    "            s_t1: next state\n",
    "\n",
    "            phi_t: current encoded state\n",
    "            phi_t1: next encoded state\n",
    "\n",
    "            a_t: current action\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "        # Stores the info from the training process\n",
    "        info = {\n",
    "            'epochs': [],\n",
    "            'scores': [],  \n",
    "            'losses': [],\n",
    "        }\n",
    "        # Stores the total reward or scores per epoch\n",
    "        scores = []\n",
    "\n",
    "        # The replay buffer for Q-learning with experience replay\n",
    "        self.replay = deque(maxlen=218)\n",
    "\n",
    "        for epoch in range(self.num_epoch):\n",
    "          \n",
    "          # play an episode\n",
    "          self.play()\n",
    "          \n",
    "          statesT = torch.stack([s.float() for s in self.states[0:-1]]) # last value is not needed here\n",
    "          statesT1 = torch.stack([s.float() for s in self.states[1:]])\n",
    "          \n",
    "          actionT = torch.FloatTensor(self.actions)\n",
    "          # convert the action tensor into one-hot\n",
    "          actionT1_hot = torch.zeros(actionT.shape[0],self.numActionSpace).scatter_(1, actionT.long().view(-1,1),1)\n",
    "          \n",
    "          if self.is_cuda:\n",
    "            statesT = statesT.cuda()\n",
    "            statesT1 = statesT1.cuda()\n",
    "            actionT = actionT.cuda()\n",
    "            actionT1_hot = actionT1_hot.cuda()\n",
    "          \n",
    "          # reset LSTM hidden states\n",
    "          #self.a2cNet.feat_enc_net.reset_lstm(s_t.shape[0])  # specify size \n",
    "\n",
    "          # call the ICM model         \n",
    "          self.icmNet.featureEncoderNet.reset_lstm(statesT.shape[0])\n",
    "          phi_t1, phi_t1_pred, a_t_pred = self.icmNet(statesT, statesT1, actionT1_hot)\n",
    "\n",
    "\n",
    "          # calculate losses\n",
    "          self.optimizer.zero_grad()\n",
    "          \n",
    "          # forward loss\n",
    "          # discrepancy between the predicted and actual next states\n",
    "          lossForward = torch.nn.functional.mse_loss(phi_t1_pred, phi_t1)\n",
    "          \n",
    "          # inverse loss\n",
    "          # cross entropy between the predicted and actual actions\n",
    "          lossInverse = torch.nn.functional.cross_entropy(a_t_pred, actionT.long().view(-1))\n",
    "          \n",
    "          # a2c loss\n",
    "          # loss of the policy (how good can we choose the proper action)\n",
    "          # and the advantage function (how good is the estimate of the value \n",
    "          # of the current state)\n",
    "          lossA2C = self.a2c_loss()\n",
    "\n",
    "          \n",
    "          # compose losses\n",
    "          loss = BETA * lossForward + (1-BETA) * lossInverse + LAMBDA * lossA2C\n",
    "\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(self.a2cNet.parameters(), GRADIENT_CLIPPING_A2C)\n",
    "          torch.nn.utils.clip_grad_norm_(self.icmNet.parameters(), GRADIENT_CLIPPING_ICM)\n",
    "          self.optimizer.step()\n",
    "          \n",
    "          # Store the total score for the episode\n",
    "          scores.append(self.score)\n",
    "\n",
    "          if epoch % np.round(self.num_epoch/10) == 0:\n",
    "                print('episode: {:d}, loss: {}, score: {:.2f}'.format(epoch, loss, scores[epoch]))\n",
    "          info[\"epochs\"].append(epoch)\n",
    "          info[\"scores\"].append(scores[epoch])\n",
    "          info[\"losses\"].append(loss.item())\n",
    "        return info\n",
    "\n",
    "      \n",
    "\n",
    "    def test(self, render=True):\n",
    "          np.random.seed(42)\n",
    "\n",
    "          self.a2cNet.featureEncoderNet.reset_lstm(1)\n",
    "          \n",
    "          # Reset the environment for testing\n",
    "          state, _ = self.env.reset()\n",
    "          # Flag to determine if an episode has ended\n",
    "          done = False\n",
    "          # Maximum number of moves allowed while testing\n",
    "          maxMoves = self.maxMoves\n",
    "          # Stores the total score received during an episode\n",
    "          score = 0\n",
    "          # Continues an episode until it ends or the maximim number of allowed moves has expired\n",
    "          while not done and maxMoves > 0:            \n",
    "              # Decrement the maximim number of allowed moves per play in an episode\n",
    "              maxMoves -= 1\n",
    "              # If render is true, renders the game to screen\n",
    "              if render:\n",
    "                  self.env.render()\n",
    "              # Calculates the probs. for the actions given a state\n",
    "              policy, value = self.a2cNet(torch.from_numpy(\n",
    "                        state).float().unsqueeze(0)) # select action from the policy\n",
    "            \n",
    "              action = np.random.choice(self.numActionSpace, p=policy[0].detach().numpy())\n",
    "            \n",
    "              # Executes an action in the environment\n",
    "              state, reward, done, truncated, _ = self.env.step(action)\n",
    "              # Stores the reward\n",
    "              score += reward\n",
    "          # Print the rewards received         \n",
    "          print('reward: {}'.format(score))\n",
    "\n",
    "\n",
    "\n",
    "    def plot(self, info_):\n",
    "        # Sort the scores by epoch\n",
    "        #info_.sort(axis=1)\n",
    "        # Extract the epochs and respective scores\n",
    "        epochs, scores, loss = info_[\"epochs\"], info_[\"scores\"], info_[\"losses\"]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "        ax1.plot(epochs, scores)\n",
    "        ax1.set_title('Scores')\n",
    "        ax1.set_xlabel('episode')\n",
    "        ax1.set_ylabel('score')\n",
    "\n",
    "        ax2.plot(epochs, loss)\n",
    "        ax2.set_title('Losses')\n",
    "        ax2.set_xlabel('episode')\n",
    "        ax2.set_ylabel('loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, loss: 3.4890923500061035, score: 0.00\n",
      "episode: 50, loss: 79.44393920898438, score: 3.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(env\u001b[38;5;241m=\u001b[39menv, num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plot the scores\u001b[39;00m\n\u001b[0;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot(info)\n",
      "Cell \u001b[1;32mIn[133], line 167\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m218\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epoch):\n\u001b[0;32m    165\u001b[0m   \n\u001b[0;32m    166\u001b[0m   \u001b[38;5;66;03m# play an episode\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m   statesT \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([s\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]) \u001b[38;5;66;03m# last value is not needed here\u001b[39;00m\n\u001b[0;32m    170\u001b[0m   statesT1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([s\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[\u001b[38;5;241m1\u001b[39m:]])\n",
      "Cell \u001b[1;32mIn[133], line 64\u001b[0m, in \u001b[0;36mAgent.play\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxMoves):\n\u001b[1;32m---> 64\u001b[0m     policy, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma2cNet(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# select action from the policy\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumActionSpace, p\u001b[38;5;241m=\u001b[39mpolicy[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# interact with the environment\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "agent = Agent(env=env, num_epoch=500)\n",
    "\n",
    "info = agent.train()\n",
    "\n",
    "# Plot the scores\n",
    "agent.plot(info)\n",
    "\n",
    "agent.test(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
