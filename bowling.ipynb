{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium ale-py\n",
    "%pip install gymnasium[atari]\n",
    "%pip install gymnasium[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ale_py\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 10\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.clamp(x, -10, 10)\n",
    "        return torch.softmax(self.fc4(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.env.seed = torch.seed\n",
    "        self.state_size = np.prod(env.observation_space.shape)\n",
    "        self.action_size = env.action_space.n\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.actor = Actor(self.state_size, self.action_size).to(self.device)\n",
    "        self.critic = Critic(self.state_size).to(self.device)\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.0001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.0001)\n",
    "\n",
    "        # Learning rate schedulers\n",
    "        self.actor_scheduler = StepLR(self.actor_optimizer, step_size=50, gamma=0.9)\n",
    "        self.critic_scheduler = StepLR(self.critic_optimizer, step_size=50, gamma=0.9)\n",
    "\n",
    "        # Exploration and other parameters (optimal : epsilon = 0.45, epsilon_decay=0.96, lr = 0.0001, gamma = 0.97)\n",
    "        self.gamma = 0.97\n",
    "        self.epsilon = 0.35 # Initial exploration\n",
    "        self.epsilon_decay = 0.97\n",
    "        \n",
    "        # Decay rate of exploration\n",
    "        self.epsilon_min = 0.01  # Minimum exploration\n",
    "\n",
    "        # Sliding window for tracking the last 10 scores\n",
    "        self.scores_window = deque(maxlen=10)\n",
    "        \n",
    "        # Directory for saving models\n",
    "        self.save_dir = './saved_models'\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def save_model(self, epoch, mean_score):\n",
    "        \"\"\"Saves both the actor and critic models to the specified directory.\"\"\"\n",
    "        model_path = os.path.join(self.save_dir, f\"actor_critic_epoch_{epoch}_mean_score_{mean_score:.2f}.pth\")\n",
    "        print(f\"Saving model to {model_path}\")\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'mean_score': mean_score\n",
    "        }, model_path)\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Charge les poids de l'actor et du critic depuis le fichier sauvegardé.\"\"\"\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        # Optionnel : si vous voulez reprendre l'entraînement, vous pouvez aussi charger les optimizers\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)  # Random action for exploration\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        probs = self.actor(state).detach().cpu().numpy()[0]\n",
    "        action = np.random.choice(self.action_size, p=probs)\n",
    "        return action\n",
    "\n",
    "    def train(self, epochs=200, max_moves=5000, batch_size=128, render=False):\n",
    "        scores = []\n",
    "        buffer = deque(maxlen=1000)  # Buffer for storing transitions\n",
    "        for e in range(epochs):\n",
    "                state, _ = self.env.reset()\n",
    "                state = state.flatten()\n",
    "                done = False\n",
    "                score = 0\n",
    "                max_moves_remaining = max_moves\n",
    "\n",
    "                while not done and max_moves_remaining > 0:\n",
    "                    max_moves_remaining -= 1\n",
    "                    action = self.get_action(state)\n",
    "                    next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                    next_state = next_state.flatten()\n",
    "\n",
    "                    # Store the experience in the buffer\n",
    "                    buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                    state = next_state\n",
    "                    score += reward\n",
    "\n",
    "                # Update by batch\n",
    "                if len(buffer) >= batch_size:\n",
    "                    transitions = random.sample(buffer, batch_size)\n",
    "                    states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "                    states_tensor = torch.from_numpy(np.array(states)).float().to(self.device)\n",
    "                    next_states_tensor = torch.from_numpy(np.array(next_states)).float().to(self.device)\n",
    "                    actions_tensor = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "                    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "                    dones_tensor = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                    # Calculate values and advantages\n",
    "                    values = self.critic(states_tensor)\n",
    "                    next_values = self.critic(next_states_tensor).detach()\n",
    "                    advantages = rewards_tensor + self.gamma * next_values * (1 - dones_tensor) - values\n",
    "\n",
    "                    probs = self.actor(states_tensor)\n",
    "                    log_probs = torch.log(probs[range(batch_size), actions_tensor])\n",
    "                    actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "\n",
    "                    # Calculate critic loss\n",
    "                    critic_loss = (advantages.pow(2)).mean()\n",
    "\n",
    "                    # Update actor and critic with gradient clipping\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    self.actor_optimizer.step()\n",
    "\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    self.critic_optimizer.step()\n",
    "\n",
    "                # Decay epsilon to reduce exploration\n",
    "                if self.epsilon > self.epsilon_min:\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "\n",
    "                # Add score to the sliding window and calculate the mean\n",
    "                self.scores_window.append(score)\n",
    "                mean_score = np.mean(self.scores_window)\n",
    "\n",
    "\n",
    "                scores.append(score)\n",
    "                print(f'Epoch: {e+1}/{epochs}, Score: {score}, Mean Score (Last 10): {mean_score:.2f}, Actor Loss: {actor_loss.item()}, Critic Loss: {critic_loss.item()}')\n",
    "                \n",
    "                # Check if the stopping condition is met (mean_score >= 75)\n",
    "                if mean_score >= 75:\n",
    "                    self.save_model(e, mean_score)\n",
    "                    break\n",
    "                    \n",
    "                # Step the schedulers to update the learning rate\n",
    "                self.actor_scheduler.step()\n",
    "                self.critic_scheduler.step()\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def test(self, episodes=10, max_moves=5000, render=True):\n",
    "        for e in range(episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            state = state.flatten()\n",
    "            done = False\n",
    "            score = 0\n",
    "            max_moves_remaining = max_moves\n",
    "            while not done and max_moves_remaining > 0:\n",
    "                max_moves_remaining -= 1\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                next_state = next_state.flatten()\n",
    "                score += reward\n",
    "                state = next_state\n",
    "            print(f'Test Episode: {e+1}/{episodes}, Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Après l'entraînement ou lors du test\n",
    "model_path = '/kaggle/working/saved_models/actor_critic_epoch_75_mean_score_75.60.pth'  # Chemin vers votre modèle sauvegardé\n",
    "agent.load_model(model_path)\n",
    "\n",
    "# Ensuite, vous pouvez tester l'agent sur de nouvelles parties\n",
    "print(\"Testing the agent...\")\n",
    "agent.test(episodes=20, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT605",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
